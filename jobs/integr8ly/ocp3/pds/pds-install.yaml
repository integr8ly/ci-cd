---

- job:
    name: pds-install
    project-type: pipeline
    description: "Installs Integreatly by using bastion server as Jenkins slave and executing Ansible installation playbook there."
    sandbox: false
    concurrent: true
    parameters:
        - string:
            name: REPOSITORY
            default: https://github.com/integr8ly/installation.git
            description: "Repository of the Integreatly installer"
        - string:
            name: BRANCH
            default: 'master'
            description: "Branch of the installer repository"
        - string:
            name: YOURCITY
            description: "City or Customer (5 char min.) plus the generated hash, e.g. qebrno-5d10 [required]"
        - string:
            name: CLUSTER_DOMAIN
            default: 'open.redhat.com'
            description: "Cluster domain - could be open.redhat.com or example.opentlc.com"
        - string:
            name: GH_CLIENT_ID
            description: "GitHub Client ID for OAuth Apps, required for some of the walkthroughs. Can be left empty"
        - string:
            name: GH_CLIENT_SECRET
            description: "GitHub Client Secret for OAuth Apps, required for some of the walkthroughs. Can be left empty"
        - bool:
            name: SELF_SIGNED_CERTS
            default: false
            description: "Indicates whether cluster uses self signed certificates or not"
        - string:
            name: ANSIBLE_USER
            default: ec2-user
            description: "User for Ansible to access the master node of target cluster"
        - string:
            name: BASTION_USER
            default: ec2-user
            description: "User capable of SSH-ing to bastion server"
        - string:
            name: BASTION_PRIVATE_KEY_ID
            default: pds-bastion-pem
            description: "ID of SSH Credentials (private key) used for SSH-ing to bastion"
        - string:
            name: ADDITIONAL_ANSIBLE_PARAMS
            default: '-e amq_streams=true'
            description: "Additional parameters passed to install playbook, e.g.'-e eval_seed_users_count=0'. Can be left empty"
        - bool:
            name: PATCH_TO_MASTER
            default: false
            description: "Indicates whether the components should be patched to master during installation"
        - bool:
            name: INSTALL_BACKUPS
            default: false
            description: "Indicates whether backups namespace should be installed during Integr8ly installation"
        - string:
            name: NAMESPACE_PREFIX
            description: "This value will be prefixed to the names of the namespaces created during Integr8ly installation. Defaulting to empty string for RHPDS"
    dsl: |
        import hudson.model.*
        import jenkins.model.*
        import hudson.slaves.*
        import hudson.slaves.EnvironmentVariablesNodeProperty.Entry

        import hudson.plugins.sshslaves.verifiers.*

        String[] intlyCompTags = ["rhsso_operator_release_tag", 
                                  "msbroker_release_tag", 
                                  "gitea_operator_release_tag",
                                  "webapp_version",
                                  "webapp_operator_release_tag",
                                  "middleware_monitoring_operator_release_tag",
                                  "backup_version"]
        String MASTER_URL = "master1.${YOURCITY}.internal"
        String BASTION_URL = "bastion.${YOURCITY}.${CLUSTER_DOMAIN}"    
        String bastionLabel = "${BASTION_URL}-slave"   
        String secretFileName = "QE-AWS-SECRET.yml"                   
        def secretContent
        try {
            timeout(75) { ansiColor('gnome-terminal') { timestamps {
                node('cirhos_rhel7') {       
                    stage('Verify input') {
                        if (!YOURCITY) {
                            throw new hudson.AbortException('YOURCITY parameter is required!')
                        }
                    }
                    String description = "domain name: ${CLUSTER_DOMAIN}<br>install branch: ${BRANCH}<br>" + 
                        "self signed certs: ${SELF_SIGNED_CERTS}<br>patch to master: ${PATCH_TO_MASTER}"
                    if (currentBuild.getBuildCauses('hudson.model.Cause$UserIdCause')['userId']){
                        description += "<br>triggered by: " + currentBuild.getBuildCauses('hudson.model.Cause$UserIdCause')['userId']
                    }
                    currentBuild.displayName = "${currentBuild.displayName} ${YOURCITY}"
                    currentBuild.description = description

                    stage('Configure bastion as jenkins slave') {
                        
                        if(Jenkins.instance.getNode(bastionLabel)) {
                            println "Slave '${bastionLabel} already exists, skipping its creation."
                        } else {
                        
                            ComputerLauncher launcher = new hudson.plugins.sshslaves.SSHLauncher(
                                "${BASTION_URL}", // Host
                                22, // Port
                                "${BASTION_PRIVATE_KEY_ID}", // Credentials
                                (String)null, // JVM Options
                                (String)null, // JavaPath
                                (hudson.tools.JDKInstaller)null, //jdkInstaller
                                (String)null, // Prefix Start Slave Command
                                (String)null, // Suffix Start Slave Command
                                (Integer)null, // Launch Timeout in Seconds
                                (Integer)null, // Maximum Number of Retries
                                (Integer)null, // The number of seconds to wait between retries
                                new NonVerifyingKeyVerificationStrategy() // Host Key Verification Strategy
                            )
                            
                            // Define a "Permanent Agent"
                            Slave agent = new DumbSlave(
                                    bastionLabel,
                                    "/home/${BASTION_USER}",
                                    launcher)
                            agent.nodeDescription = "Bastion for ${JOB_NAME}"
                            agent.numExecutors = 3
                            agent.labelString = bastionLabel
                            agent.mode = Node.Mode.EXCLUSIVE
                            agent.retentionStrategy = new RetentionStrategy.Always()
                
                            // Create a "Permanent Agent"
                            Jenkins.instance.addNode(agent)
                            
                            println "Slave ${bastionLabel} has been created successfully."
                        } // end if
                    } // stage

                    stage('Obtain s3-credentials secret for backups namespace') {
                        if (INSTALL_BACKUPS.toString() == 'true') {
                            secretContent = sh (returnStdout: true, script: 'curl -k https://gitlab.cee.redhat.com/integreatly-qe/integreatly-qe/raw/development/resources/QE-AWS-SECRET.yml')
                        }
                    } // stage
                } // node

                node(bastionLabel) {
                    stage('Clone the installer') {
                        dir('installation') {
                            checkout([
                                $class: 'GitSCM',
                                branches: [[name: BRANCH]],
                                doGenerateSubmoduleConfigurations: false,
                                extensions: [],
                                userRemoteConfigs: [[url: REPOSITORY]]
                            ])

                            sh "git clean -xdf"

                            if(!fileExists("evals")) {
                                sh "ln -s . evals"
                            }
                        } // dir
                    } // stage

                    stage('Prepare environment') {
                        dir('installation') {
                            sh """
                                sudo oc login -u system:admin
                                cp ./evals/inventories/hosts.template ./evals/inventories/hosts
                                sed -i 's/ansible_user=ec2-user/ansible_user=${ANSIBLE_USER}/g' ./evals/inventories/hosts
                            """
                            
                            // not needed - there is only one master node on PDS
                            // String masterUrls = MASTER_URLS.replaceAll(~/,[\s]*/, '\\\\n')
                            
                            sh """
                                sed -i '\$!N;s@\\[master\\]\\n127.0.0.1@[master]\\n${MASTER_URL}@;P;D' ./evals/inventories/hosts
                                sed -i '\$!N;s@\\[master\\]\\nmaster.evals.example.com@[master]\\n${MASTER_URL}@;P;D' ./evals/inventories/hosts
                            """
                            
                            String output = readFile('./evals/inventories/hosts')
                            println output
                            
                            if (BRANCH == 'master' && PATCH_TO_MASTER.toString() == 'true') {   
                                // patch all the integreatly components to use master tag version
                                for (String tag in intlyCompTags) {
                                    sh """
                                        sed -i "/${tag}: /c\\${tag}: 'master'" inventories/group_vars/all/manifest.yaml
                                    """
                                }
                            }
                            if (INSTALL_BACKUPS.toString() == 'true') {
                                String backupsNamespace = 'openshift-integreatly-backups';
                                String secretName = 's3-credentials';
                                String obtainSecretsCommand = 'sudo oc get secrets -n ' + backupsNamespace

                                def projects = sh(script: 'sudo oc projects', returnStdout: true)
                                if (!projects.contains(backupsNamespace)) {
                                    sh """
                                        sudo oc create ns ${backupsNamespace}
                                    """
                                }
                                
                                def secrets = sh(script: obtainSecretsCommand, returnStdout: true)
                                if (!secrets.contains(secretName)) {
                                    writeYaml file: secretFileName, data: secretContent
                                    sh """
                                        sudo oc create -f ${secretFileName}
                                        rm ${secretFileName}
                                    """
                                }
                            }
                        } // dir
                    } // stage
                    
                    stage('Execute playbook') {
                        dir('installation/evals') {
                            
                            String ansibleParams = ''
                            if (GH_CLIENT_ID && GH_CLIENT_SECRET) {
                                ansibleParams = "-e github_client_id=${GH_CLIENT_ID} -e github_client_secret=${GH_CLIENT_SECRET}"
                            }
                            if (ADDITIONAL_ANSIBLE_PARAMS.indexOf('eval_seed_users_count') == -1) {
                                ansibleParams = ansibleParams + " -e eval_seed_users_count=15"
                            }
                            if (INSTALL_BACKUPS.toString() == 'true') {
                                ansibleParams = ansibleParams + " -e backup_restore_install=true"
                            }
                            ansibleParams = ansibleParams + " ${ADDITIONAL_ANSIBLE_PARAMS}"
                            
                            sh """
                                sudo ansible-playbook -i ./inventories/hosts ./playbooks/install.yml ${ansibleParams} -e eval_self_signed_certs=${SELF_SIGNED_CERTS} -e ns_prefix=${NAMESPACE_PREFIX}
                            """
                        } // dir
                    } // stage

                    stage('Execute "After Installation" Workarounds') {
                        if (BRANCH == 'master' && PATCH_TO_MASTER.toString() == 'true') {  
                            sleep time: 2, unit: 'MINUTES' // to make sure that everything is ready to be patched
                            sh """
                                # patch webapp tutorial and operator
                                sudo oc patch webapp tutorial-web-app-operator -n ${NAMESPACE_PREFIX}webapp --type=merge -p '{ "spec": { "template": { "parameters": { "WALKTHROUGH_LOCATIONS": "https://github.com/integr8ly/tutorial-web-app-walkthroughs.git#master" }}}}' || true
                                sudo oc patch deployment tutorial-web-app-operator -n ${NAMESPACE_PREFIX}webapp -p '{ "spec": { "template": { "spec": { "containers": [{ "name": "tutorial-web-app-operator", "image": "quay.io/integreatly/tutorial-web-app-operator:master" }]}}}}' || true

                                # patch of tutorial-web-app, workaround for https://issues.jboss.org/browse/INTLY-1201
                                sudo oc patch deploymentconfig/tutorial-web-app -n ${NAMESPACE_PREFIX}webapp -p '{ "spec": { "template": { "spec": { "containers": [{ "name": "tutorial-web-app", "image": "quay.io/integreatly/tutorial-web-app:master" }]}}}}' || true
                                
                                # patch grafana-operator to use master tag
                                # Commented out. The master branch of Grafana operator now contains modifications for OCPv4. We can't just use master anymore on OCPv3
                                # 
                                # sudo oc scale --replicas=0 deployment grafana-operator -n ${NAMESPACE_PREFIX}middleware-monitoring
                                # sudo oc patch deployment grafana-operator -n ${NAMESPACE_PREFIX}middleware-monitoring -p  '{ "spec": { "template": { "spec": { "containers": [{ "name": "grafana-operator", "image": "quay.io/integreatly/grafana-operator:master" }]}}}}'
                                # sudo oc scale --replicas=1 deployment grafana-operator -n ${NAMESPACE_PREFIX}middleware-monitoring
                                
                                # patch application-monitoring-operator to use master tag
                                sudo oc scale --replicas=0 deployment application-monitoring-operator -n ${NAMESPACE_PREFIX}middleware-monitoring
                                sudo oc patch deployment application-monitoring-operator -n ${NAMESPACE_PREFIX}middleware-monitoring -p  '{ "spec": { "template": { "spec": { "containers": [{ "name": "application-monitoring-operator", "image": "quay.io/integreatly/application-monitoring-operator:master" }]}}}}'
                                sudo oc scale --replicas=1 deployment application-monitoring-operator -n ${NAMESPACE_PREFIX}middleware-monitoring
                                
                                # patch keycloak-operator to use master tag
                                sudo oc patch deployment keycloak-operator -n ${NAMESPACE_PREFIX}sso -p  '{ "spec": { "template": { "spec": { "containers": [{ "name": "keycloak-operator", "image": "quay.io/integreatly/keycloak-operator:master" }]}}}}'

                                # patch user-sso keycloak-operator to use master tag
                                sudo oc patch deployment keycloak-operator -n ${NAMESPACE_PREFIX}user-sso -p  '{ "spec": { "template": { "spec": { "containers": [{ "name": "keycloak-operator", "image": "quay.io/integreatly/keycloak-operator:master" }]}}}}'
                            """
                            sleep time: 5, unit: 'MINUTES' // to make sure that webapp is up and running after patching is applied
                        }
                    } // stage
                } // node
            }}} // timeout, ansiColor, timestamps

        } finally {
            // Clean up - remove jenkins slave that was created
        
            Node bastionSlave = Jenkins.instance.getNode(bastionLabel);
            if(bastionSlave != null) {
                Boolean isIdle = bastionSlave.toComputer().isIdle();
                if(isIdle) {
                    Jenkins.instance.removeNode(bastionSlave);
                }
            }
        }
